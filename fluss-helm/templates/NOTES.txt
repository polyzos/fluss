Thank you for installing Fluss!

Release: {{ .Release.Name }}
Namespace: {{ .Release.Namespace }}

Coordinator Service:
  Name: {{ include "fluss.coordinator.serviceName" . }}
  Port: {{ .Values.coordinator.service.port }}

{{- if .Values.coordinator.externalService.enabled }}
External Coordinator Service:
  - Mode: {{ .Values.coordinator.externalService.type | default "NodePort" }}
  - Name: {{ include "fluss.fullname" . }}-coordinator-external
  {{- if eq (default "NodePort" .Values.coordinator.externalService.type) "NodePort" }}
  - Connect from outside K8s using your node IP (e.g., Minikube):
      Host: $(minikube ip)
      Port: {{ .Values.coordinator.externalService.nodePort | default 30912 }}
  {{- else }}
  - LoadBalancer: check 'kubectl get svc {{ include "fluss.fullname" . }}-coordinator-external' for external IP/hostname.
  {{- end }}
{{- end }}

Tablet Headless Service:
  Name: {{ include "fluss.tablet.headlessServiceName" . }}
  Port: {{ .Values.tablet.service.port }}

{{- if .Values.zookeeper.enabled }}
ZooKeeper Service:
  Name: {{ include "fluss.zookeeper.serviceName" . }}
  Port: {{ .Values.zookeeper.service.port }}
{{- end }}

Client connection (inside cluster):
  Use the coordinator's CLIENT listener advertised at:
  {{- $svc := printf "%s.%s.svc" (include "fluss.coordinator.serviceName" .) .Release.Namespace }}
  - {{ $svc }}:{{ .Values.coordinator.service.port }}

Advertised listeners:
  - Coordinator advertised.listeners: {{ index .Values.coordinator.config "advertised.listeners" | default "(inherits bind.listeners)" }}
  - Tablet advertised.listeners: {{ index .Values.tablet.config "advertised.listeners" | default "(inherits bind.listeners)" }}

{{- if .Values.tablet.externalService.enabled }}
Tablet external Services:
  - Mode: {{ .Values.tablet.externalService.type | default "NodePort" }}
  - One Service per tablet pod is created: {{ include "fluss.fullname" . }}-tablet-<ordinal>
  - Base port: {{ .Values.tablet.externalService.portBase | default 30090 }} (tablet i -> base + i)
  - Services created:
  {{- $replicas := int .Values.tablet.replicas }}
  {{- $fullname := include "fluss.fullname" . }}
  {{- $base := default 30090 .Values.tablet.externalService.portBase }}
  {{- range $i, $_ := until $replicas }}
    - {{ $fullname }}-tablet-{{ $i }} (port: {{ add $base $i }})
  {{- end }}
  {{- if eq (default "NodePort" .Values.tablet.externalService.type) "NodePort" }}
  - Connect from outside K8s using your node IP (e.g., Minikube):
      Host: $(minikube ip)
      Ports: base + ordinal (e.g., 30090, 30091, 30092)
  {{- else }}
  - LoadBalancer: check 'kubectl get svc' for external IPs/hostnames.
  {{- end }}
{{- end }}

{{- if .Values.tablet.externalAccess.enabled }}
Tablet advertised endpoints override:
  - Host source: {{ .Values.tablet.externalAccess.hostSource | default "value" }}
  - Host value: {{ .Values.tablet.externalAccess.host | default "(nodeIP when hostSource=nodeIP)" }}
  - Port base: {{ .Values.tablet.externalAccess.portBase | default 19000 }} (tablet i -> base + i)
{{- end }}

Datalake (Paimon) Warehouse:
{{- if .Values.datalake.enabled }}
  - Status: enabled
  - Warehouse path: {{ index .Values.coordinator.config "datalake.paimon.warehouse" | default "/var/lib/paimon" }}
  {{- if .Values.datalake.warehousePersistence.existingClaim }}
  - PVC: {{ .Values.datalake.warehousePersistence.existingClaim }} (ReadWriteMany expected)
  {{- else if .Values.datalake.warehousePersistence.enabled }}
  - PVC: {{ include "fluss.fullname" . }}-paimon-warehouse (auto-created, RWX)
  {{- else }}
  - Volume: EmptyDir (dev-only). Warning: not shared between pods; coordinator and tablets will NOT see the same tables.
  {{- end }}
{{- else }}
  - Status: disabled. To use tables with table.datalake.enabled=true, enable a shared warehouse:
      --set datalake.enabled=true \
      --set datalake.warehousePersistence.enabled=true \
      --set datalake.warehousePersistence.storageClass=<rwx-class>
{{- end }}

Security (SASL):
  If enabled, ensure clients configure matching JAAS credentials and security.protocol.

Image pull issues (Minikube):
  If pods are stuck in ImagePullBackOff, build the image locally and install with:
  - eval $(minikube docker-env)
  - docker build -t fluss:0.7 docker
  - helm upgrade --install fluss ./fluss-helm --set image.repository=fluss --set image.tag=0.7 --set image.pullPolicy=Never

To get pod FQDNs for tablets:
  kubectl get pod -l app.kubernetes.io/instance={{ .Release.Name }},app.kubernetes.io/component=tablet -o custom-columns=NAME:.metadata.name,FQDN:.metadata.name --no-headers | awk '{print $1".""{{ include "fluss.tablet.headlessServiceName" . }}"".""{{ .Release.Namespace }}"".svc"}'

